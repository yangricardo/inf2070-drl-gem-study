{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2a21aa6",
   "metadata": {},
   "source": [
    "# Algoritmos de Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7498f589",
   "metadata": {},
   "source": [
    "## REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf25febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from xlsx_generator import XlsxGenerator\n",
    "# from ..xlsx_read import XlsxReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d87675a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando treinamento com REINFORCE...\n",
      "Episódio 100/1000, Recompensa Total: 207.0\n",
      "Episódio 200/1000, Recompensa Total: 500.0\n",
      "Episódio 300/1000, Recompensa Total: 18.0\n",
      "Episódio 400/1000, Recompensa Total: 500.0\n",
      "Episódio 500/1000, Recompensa Total: 500.0\n",
      "Episódio 600/1000, Recompensa Total: 164.0\n",
      "Episódio 700/1000, Recompensa Total: 500.0\n",
      "Episódio 800/1000, Recompensa Total: 224.0\n",
      "Episódio 900/1000, Recompensa Total: 402.0\n",
      "Episódio 1000/1000, Recompensa Total: 500.0\n",
      "Treinamento REINFORCE concluído.\n"
     ]
    }
   ],
   "source": [
    "# --- Hiperparâmetros ---\n",
    "LEARNING_RATE = 0.01\n",
    "GAMMA = 0.99         # Fator de desconto para recompensas futuras\n",
    "N_EPISODES = 1000\n",
    "\n",
    "\n",
    "# --- Passo 1: Definir a Rede de Política (O \"Ator\") ---\n",
    "# Esta rede neural decide qual ação tomar dado um estado.\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        # 4 entradas (estado) -> 128 neurônios -> 2 saídas (logits da ação)\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, action_dim) # Saída são os logits\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        logits = self.fc2(x)\n",
    "        return logits\n",
    "    \n",
    "# --- Passo 2: Função para selecionar ação e calcular log_prob ---\n",
    "def select_action(network, state):\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    logits = network(state)\n",
    "    \n",
    "    # Cria uma distribuição categórica (ex: [0.6, 0.4]) a partir dos logits\n",
    "    # O PyTorch aplica o softmax internamente\n",
    "    dist = Categorical(logits=logits)\n",
    "    \n",
    "    # Amostra uma ação da distribuição (Exploração!)\n",
    "    action = dist.sample()\n",
    "    \n",
    "    # Calcula o log(prob) da ação que foi amostrada.\n",
    "    # Isso é o log(pi(a|s)) que precisamos para a perda!\n",
    "    log_prob = dist.log_prob(action)\n",
    "    \n",
    "    return action.item(), log_prob\n",
    "    \n",
    "# --- Passo 3: Função para calcular os Retornos Descontados (G_t) ---\n",
    "def calculate_discounted_returns(rewards):\n",
    "    \"\"\"\n",
    "    Calcula o \"reward-to-go\" (G_t) para cada passo no episódio.\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    G_t = 0 # Retorno acumulado\n",
    "    \n",
    "    # Itera de trás para frente sobre as recompensas\n",
    "    for r in reversed(rewards):\n",
    "        # G_t = r_t + gamma * G_t+1\n",
    "        G_t = r + GAMMA * G_t\n",
    "        returns.insert(0, G_t) # Insere no início da lista\n",
    "        \n",
    "    # Converte para tensor\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "    \n",
    "    # [TRUQUE DE ESTABILIZAÇÃO]: Normalizar os retornos\n",
    "    # Isso centraliza os retornos em torno de 0.\n",
    "    # Vantagens \"boas\" se tornam > 0, \"ruins\" < 0.\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-9) # Evita divisão por zero\n",
    "    \n",
    "    return returns\n",
    "\n",
    "# --- Passo 4: O Loop de Treinamento ---\n",
    "\n",
    "# Inicializa o ambiente e a rede\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "policy_net = PolicyNetwork(state_dim, action_dim)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Iniciando treinamento com REINFORCE...\")\n",
    "\n",
    "episode_details = []\n",
    "\n",
    "for episode in range(N_EPISODES):\n",
    "    # Listas para armazenar dados do episódio\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0\n",
    "\n",
    "    # --- 1. Coletar um episódio completo (Rollout) ---\n",
    "    while not (terminated or truncated):\n",
    "        # Seleciona a ação e o log_prob\n",
    "        action, log_prob = select_action(policy_net, state)\n",
    "        \n",
    "        # Executa a ação no ambiente\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode_details.append({\n",
    "            \"episode\": episode + 1,\n",
    "            \"step_reward\": reward,            \n",
    "            \"total_reward\": total_reward + reward,\n",
    "            \"action\": action,\n",
    "            'terminated': terminated,\n",
    "            'truncated': truncated,\n",
    "            'next_state': next_state,\n",
    "            'reward': reward,\n",
    "            'data': _,\n",
    "            'type': 'step'\n",
    "        })\n",
    "        # Armazena os dados\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    \n",
    "    # --- 2. Calcular Retornos ---\n",
    "    returns = calculate_discounted_returns(rewards)\n",
    "\n",
    "    # --- 3. Calcular a Perda (Loss) ---\n",
    "    policy_loss = []\n",
    "    # Itera sobre cada passo (log_prob, G_t)\n",
    "    for log_prob, G_t in zip(log_probs, returns):\n",
    "        # A perda é -log(pi(a|s)) * G_t\n",
    "        # O sinal negativo é porque o otimizador TENTA MINIMIZAR.\n",
    "        # Minimizar -[log_prob * G_t] é o mesmo que MAXIMIZAR [log_prob * G_t].\n",
    "        policy_loss.append(-log_prob * G_t)\n",
    "        \n",
    "    # Soma as perdas de todos os passos do episódio\n",
    "    # Precisamos empilhar os tensores antes de somar\n",
    "    loss = torch.stack(policy_loss).sum()\n",
    "    episode_details.append({\n",
    "        \"episode\": episode + 1,\n",
    "        \"step_reward\": None,            \n",
    "        \"total_reward\": total_reward,\n",
    "        \"action\": None,\n",
    "        'terminated': terminated,\n",
    "        'truncated': truncated,\n",
    "        'next_state': None,\n",
    "        'reward': None,\n",
    "        'data': None,\n",
    "        'type': 'loss',\n",
    "        'loss': loss.item()\n",
    "    })\n",
    "    # --- 4. Atualizar a Rede ---\n",
    "    optimizer.zero_grad() # Zera os gradientes\n",
    "    loss.backward()       # Calcula os gradientes (Backpropagation)\n",
    "    optimizer.step()      # Aplica os gradientes\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"Episódio {episode+1}/{N_EPISODES}, Recompensa Total: {total_reward}\")\n",
    "\n",
    "env.close()\n",
    "print(\"Treinamento REINFORCE concluído.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6d5632e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yang/projects/inf2070-gym/inf2070-studies/notebooks/outputs/reinforce_training_log_20251026_165136.xlsx'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlsx = XlsxGenerator(\"outputs/reinforce_training_log.xlsx\")\n",
    "xlsx.add_dataframes(\"training_log\", episode_details)\n",
    "xlsx.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7344e59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
