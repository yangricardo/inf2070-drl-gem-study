{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20a04927",
   "metadata": {},
   "source": [
    "# Experimentação de Cenários utilizando o GEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e44ff5",
   "metadata": {},
   "source": [
    "### Importação de dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bce483f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/micael/mestrado/inf2070-drl-gem-study/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import gem\n",
    "from gem.tools.python_code_tool import PythonCodeTool\n",
    "from gem.tools.tool_env_wrapper import ToolEnvWrapper\n",
    "from gem.wrappers.wrapper_factory import WRAPPER_FACTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e252c59",
   "metadata": {},
   "source": [
    "### Implementação da ação do agente LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e026f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Função Auxiliar para Geração do LLM ---\n",
    "def generate_action(prompt, model, tokenizer, device, max_new_tokens=1024):\n",
    "    \"\"\"\n",
    "    Gera uma ação (texto) a partir de um prompt (observação) usando o LLM.\n",
    "    \"\"\"\n",
    "    # O 'obs' do concat_chat já é o prompt formatado.\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Garantir que pad_token_id está definido para geração open-ended\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Gerar a resposta\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decodificar apenas os tokens *novos* gerados, ignorando o prompt de entrada\n",
    "    input_length = inputs[\"input_ids\"].shape[1]\n",
    "    new_tokens = outputs[0, input_length:]\n",
    "    action_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    return action_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e64b4",
   "metadata": {},
   "source": [
    "### Ciclo de Interação Agente-Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ae4a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Using model: deepseek-ai/deepseek-coder-1.3b-instruct ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Initial Observation (Problem)]:\n",
      "<｜begin▁of▁sentence｜>You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\n",
      "### Instruction:\n",
      "Three siblings are born 5 years apart, each. If the eldest child is 20 years old now, what's the total of the ages of the three siblings 10 years from now?\n",
      "Solve the above problem step by step. You now have the ability to selectively write executable Python code to enhance your reasoning process. The Python code will be executed by an external sandbox, and the output (after \"Code execution result: \") is returned to aid your reasoning and help you arrive at the final answer. The Python code should be complete scripts, including necessary imports, wrapped within <python>...</python> tags or using ```python...``` code block. Return your final answer within \\boxed{}.\n",
      "### Response:\n",
      "\n",
      "[Initial Info]: {'tool_use_counter': 0, 'prev_ep_tool_use_counter': 0, 'tool_success_counter': 0, 'prev_ep_tool_success_counter': 0, 'use_tool': False}\n",
      "------------------------------\n",
      "\n",
      "[Step 1]\n",
      "\n",
      "[LLM Action]:\n",
      "The eldest sibling is 20 years old now, so the youngest sibling is 20 - 5 = 15 years old.\n",
      "\n",
      "The siblings are 5 years apart, so the second sibling will be 15 + 5 = 20 years old.\n",
      "\n",
      "The third sibling will be 20 - 5 = 15 years old.\n",
      "\n",
      "So, the total of the ages of the three siblings 10 years from now will be 15 + 15 + 15 = 45 years.\n",
      "\n",
      "Here is the Python code to solve the problem:\n",
      "\n",
      "```python\n",
      "# Define the age of the eldest sibling\n",
      "eldest_age = 20\n",
      "\n",
      "# Calculate the age of the youngest sibling\n",
      "youngest_age = eldest_age - 5\n",
      "\n",
      "# Calculate the age of the second sibling\n",
      "second_sibling_age = youngest_age + 5\n",
      "\n",
      "# Calculate the age of the third sibling\n",
      "third_sibling_age = second_sibling_age + 5\n",
      "\n",
      "# Calculate the total age of the siblings 10 years from now\n",
      "total_age_10_years_from_now = third_sibling_age + 10\n",
      "\n",
      "# Print the result\n",
      "print(\"The total of the ages of the three siblings 10 years from now is\", total_age_10_years_from_now, \"years.\")\n",
      "```\n",
      "\n",
      "When you run this code, it will print:\n",
      "\n",
      "```python\n",
      "The total of the ages of the three siblings 10 years from now is 45 years.\n",
      "```\n",
      "\n",
      "\n",
      "[Environment Feedback (Next Observation)]:\n",
      "<｜begin▁of▁sentence｜>You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\n",
      "### Instruction:\n",
      "Three siblings are born 5 years apart, each. If the eldest child is 20 years old now, what's the total of the ages of the three siblings 10 years from now?\n",
      "Solve the above problem step by step. You now have the ability to selectively write executable Python code to enhance your reasoning process. The Python code will be executed by an external sandbox, and the output (after \"Code execution result: \") is returned to aid your reasoning and help you arrive at the final answer. The Python code should be complete scripts, including necessary imports, wrapped within <python>...</python> tags or using ```python...``` code block. Return your final answer within \\boxed{}.\n",
      "### Response:\n",
      "The eldest sibling is 20 years old now, so the youngest sibling is 20 - 5 = 15 years old.\n",
      "\n",
      "The siblings are 5 years apart, so the second sibling will be 15 + 5 = 20 years old.\n",
      "\n",
      "The third sibling will be 20 - 5 = 15 years old.\n",
      "\n",
      "So, the total of the ages of the three siblings 10 years from now will be 15 + 15 + 15 = 45 years.\n",
      "\n",
      "Here is the Python code to solve the problem:\n",
      "\n",
      "```python\n",
      "# Define the age of the eldest sibling\n",
      "eldest_age = 20\n",
      "\n",
      "# Calculate the age of the youngest sibling\n",
      "youngest_age = eldest_age - 5\n",
      "\n",
      "# Calculate the age of the second sibling\n",
      "second_sibling_age = youngest_age + 5\n",
      "\n",
      "# Calculate the age of the third sibling\n",
      "third_sibling_age = second_sibling_age + 5\n",
      "\n",
      "# Calculate the total age of the siblings 10 years from now\n",
      "total_age_10_years_from_now = third_sibling_age + 10\n",
      "\n",
      "# Print the result\n",
      "print(\"The total of the ages of the three siblings 10 years from now is\", total_age_10_years_from_now, \"years.\")\n",
      "```\n",
      "<|EOT|>\n",
      "### Response:\n",
      "Code execution result: The total of the ages of the three siblings 10 years from now is 35 years.\n",
      "\n",
      "\n",
      "<|EOT|>\n",
      "### Response:\n",
      "\n",
      "[Reward]: 0.0\n",
      "[Info]: {'parsed_action': 'The eldest sibling is 20 years old now, so the youngest sibling is 20 - 5 = 15 years old.\\n\\nThe siblings are 5 years apart, so the second sibling will be 15 + 5 = 20 years old.\\n\\nThe third sibling will be 20 - 5 = 15 years old.\\n\\nSo, the total of the ages of the three siblings 10 years from now will be 15 + 15 + 15 = 45 years.\\n\\nHere is the Python code to solve the problem:\\n\\n```python\\n# Define the age of the eldest sibling\\neldest_age = 20\\n\\n# Calculate the age of the youngest sibling\\nyoungest_age = eldest_age - 5\\n\\n# Calculate the age of the second sibling\\nsecond_sibling_age = youngest_age + 5\\n\\n# Calculate the age of the third sibling\\nthird_sibling_age = second_sibling_age + 5\\n\\n# Calculate the total age of the siblings 10 years from now\\ntotal_age_10_years_from_now = third_sibling_age + 10\\n\\n# Print the result\\nprint(\"The total of the ages of the three siblings 10 years from now is\", total_age_10_years_from_now, \"years.\")\\n```', 'tool_type': 'python_code', 'tool_use_counter': 1, 'tool_success_counter': 1, 'use_tool': True}\n",
      "------------------------------\n",
      "\n",
      "[Step 2]\n",
      "\n",
      "[LLM Action]:\n",
      "The total of the ages of the three siblings 10 years from now is 35 years.\n",
      "\n",
      "\n",
      "[Environment Feedback (Next Observation)]:\n",
      "<｜begin▁of▁sentence｜>You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\n",
      "### Instruction:\n",
      "Three siblings are born 5 years apart, each. If the eldest child is 20 years old now, what's the total of the ages of the three siblings 10 years from now?\n",
      "Solve the above problem step by step. You now have the ability to selectively write executable Python code to enhance your reasoning process. The Python code will be executed by an external sandbox, and the output (after \"Code execution result: \") is returned to aid your reasoning and help you arrive at the final answer. The Python code should be complete scripts, including necessary imports, wrapped within <python>...</python> tags or using ```python...``` code block. Return your final answer within \\boxed{}.\n",
      "### Response:\n",
      "The eldest sibling is 20 years old now, so the youngest sibling is 20 - 5 = 15 years old.\n",
      "\n",
      "The siblings are 5 years apart, so the second sibling will be 15 + 5 = 20 years old.\n",
      "\n",
      "The third sibling will be 20 - 5 = 15 years old.\n",
      "\n",
      "So, the total of the ages of the three siblings 10 years from now will be 15 + 15 + 15 = 45 years.\n",
      "\n",
      "Here is the Python code to solve the problem:\n",
      "\n",
      "```python\n",
      "# Define the age of the eldest sibling\n",
      "eldest_age = 20\n",
      "\n",
      "# Calculate the age of the youngest sibling\n",
      "youngest_age = eldest_age - 5\n",
      "\n",
      "# Calculate the age of the second sibling\n",
      "second_sibling_age = youngest_age + 5\n",
      "\n",
      "# Calculate the age of the third sibling\n",
      "third_sibling_age = second_sibling_age + 5\n",
      "\n",
      "# Calculate the total age of the siblings 10 years from now\n",
      "total_age_10_years_from_now = third_sibling_age + 10\n",
      "\n",
      "# Print the result\n",
      "print(\"The total of the ages of the three siblings 10 years from now is\", total_age_10_years_from_now, \"years.\")\n",
      "```\n",
      "<|EOT|>\n",
      "### Response:\n",
      "Code execution result: The total of the ages of the three siblings 10 years from now is 35 years.\n",
      "\n",
      "\n",
      "<|EOT|>\n",
      "### Response:\n",
      "The total of the ages of the three siblings 10 years from now is 35 years.\n",
      "\n",
      "<|EOT|>\n",
      "### Response:\n",
      "<｜TERMINAL_STATE｜>\n",
      "<|EOT|>\n",
      "### Response:\n",
      "\n",
      "[Reward]: 0\n",
      "[Info]: {'correct': 0, 'tool_use_counter': 1, 'tool_success_counter': 1, 'use_tool': True}\n",
      "------------------------------\n",
      "\n",
      "========== Episode Finished ==========\n",
      "Total Reward: 0.0\n",
      "Final Info: {'correct': 0, 'tool_use_counter': 1, 'tool_success_counter': 1, 'use_tool': True}\n",
      "=== Finished using model: deepseek-ai/deepseek-coder-1.3b-instruct ===\n",
      "\n",
      "All models processed.\n"
     ]
    }
   ],
   "source": [
    "base_env = gem.make(\"math:GSM8K\")\n",
    "# Esta é a ferramenta que o LLM pode decidir usar\n",
    "tool = PythonCodeTool()\n",
    "# Este wrapper dá ao ambiente a capacidade de entender e executar a ferramenta\n",
    "tool_env = ToolEnvWrapper(base_env, tools=[tool])\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "# Lista de modelos para testar\n",
    "models = [\n",
    "    # \"microsoft/Phi-3-mini-4k-instruct\", # Usando Phi-3 no lugar do Phi-4 que não existe\n",
    "    # \"Qwen/Qwen2-0.5B-Instruct\",      # Usando Qwen2 0.5B no lugar do Qwen3 0.6B\n",
    "    \"deepseek-ai/deepseek-coder-1.3b-instruct\" # Usando um modelo de código Deepseek\n",
    "]\n",
    "\n",
    "# --- 2. Loop Principal (Iterar sobre cada modelo) ---\n",
    "for model_name in models:\n",
    "    print(f\"\\n{'='*20} Using model: {model_name} {'='*20}\")\n",
    "    \n",
    "    try:\n",
    "        # --- 3. Carregar Modelo e Tokenizer ---\n",
    "        # trust_remote_code=True é frequentemente necessário para modelos mais novos\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32 # Otimização para GPU\n",
    "        ).to(device)\n",
    "        llm_model.eval() # Definir modelo para modo de avaliação\n",
    "\n",
    "        # --- 4. Aplicar o Wrapper de Chat ---\n",
    "        # Este wrapper formata o histórico de observações em um prompt de chat\n",
    "        # que o LLM entende, incluindo a descrição das ferramentas.\n",
    "        chat_env = WRAPPER_FACTORY[\"concat_chat\"](\n",
    "            tool_env, tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "        # --- 5. Iniciar o Episódio ---\n",
    "        obs, info = chat_env.reset()\n",
    "        print(f\"[Initial Observation (Problem)]:\\n{obs}\")\n",
    "        print(f\"[Initial Info]: {info}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        # --- 6. Loop de Interação (Agente-Ambiente) ---\n",
    "        while True:\n",
    "            step_count += 1\n",
    "            print(f\"\\n[Step {step_count}]\")\n",
    "\n",
    "            # === AÇÃO DO LLM ===\n",
    "            # Em vez de uma ação aleatória, geramos uma ação com o LLM\n",
    "            with torch.no_grad(): # Desativar cálculo de gradiente para inferência\n",
    "                action = generate_action(obs, llm_model, tokenizer, device)\n",
    "            \n",
    "            # --- LOG: Ação do LLM ---\n",
    "            print(f\"\\n[LLM Action]:\\n{action}\\n\")\n",
    "            \n",
    "            # === PASSO NO AMBIENTE ===\n",
    "            # O 'chat_env' recebe a ação de texto.\n",
    "            # O 'ToolEnvWrapper' (dentro dele) irá procurar por <python>...</python>\n",
    "            # Se encontrar, executa o código e retorna o stdout/stderr.\n",
    "            # Se não encontrar, passa a ação direto para o 'base_env'.\n",
    "            next_observation, reward, terminated, truncated, info = chat_env.step(action)\n",
    "            \n",
    "            # --- LOG: Feedback do Ambiente ---\n",
    "            print(f\"[Environment Feedback (Next Observation)]:\\n{next_observation}\")\n",
    "            print(f\"[Reward]: {reward}\")\n",
    "            print(f\"[Info]: {info}\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "            # Atualizar a observação para o próximo loop\n",
    "            obs = next_observation\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Condição de parada\n",
    "            if terminated or truncated:\n",
    "                print(f\"\\n{'='*10} Episode Finished {'='*10}\")\n",
    "                print(f\"Total Reward: {episode_reward}\")\n",
    "                print(f\"Final Info: {info}\")\n",
    "                break\n",
    "            \n",
    "            # Limite de segurança (opcional, mas bom para debug)\n",
    "            if step_count > 10:\n",
    "                print(\"Step limit reached, truncating episode.\")\n",
    "                break\n",
    "\n",
    "        chat_env.close()\n",
    "        print(f\"=== Finished using model: {model_name} ===\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to run model {model_name}. Error: {e}\")\n",
    "        # Limpar memória da GPU se houver erro antes de ir para o próximo modelo\n",
    "        if 'llm_model' in locals():\n",
    "            del llm_model\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nAll models processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6f775",
   "metadata": {},
   "source": [
    "### Ciclo Interação Agente-Ambiente (sem ferramentas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93650745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Using model: TinyLlama/TinyLlama-1.1B-Chat-v1.0 ====================\n",
      "[Initial Observation (Problem)]:\n",
      "<|user|>\n",
      "You are playing Guess The Number.\n",
      "You have to guess the number between 1 and 10 (inclusive) within 4 turns.\n",
      "As you enter your guess, the game will provide you with hints such as the target number is 'higher' or 'lower'.\n",
      "You may provide your response in any manner. Only the number that is wrapped inside \\boxed{} will be considered as your guess, for example, \\boxed{10}.\n",
      "As you play, the history of your guesses will be appended below. Use the information to complete the game before you run out of guesses.\n",
      "Enter your first guess to start the game.\n",
      "</s>\n",
      "<|assistant|>\n",
      "Enter your next guess.\n",
      "[Initial Info]: {'suffix': 'Enter your next guess.'}\n",
      "------------------------------\n",
      "\n",
      "[Step 1]\n",
      "\n",
      "[LLM Action]:\n",
      "\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "\n",
      "\n",
      "[Environment Feedback (Next Observation)]:\n",
      "<|user|>\n",
      "You are playing Guess The Number.\n",
      "You have to guess the number between 1 and 10 (inclusive) within 4 turns.\n",
      "As you enter your guess, the game will provide you with hints such as the target number is 'higher' or 'lower'.\n",
      "You may provide your response in any manner. Only the number that is wrapped inside \\boxed{} will be considered as your guess, for example, \\boxed{10}.\n",
      "As you play, the history of your guesses will be appended below. Use the information to complete the game before you run out of guesses.\n",
      "Enter your first guess to start the game.\n",
      "</s>\n",
      "<|assistant|>\n",
      "\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "Enter your next guess.\n",
      "\n",
      "</s>\n",
      "<|user|>\n",
      "At turn 1, you did not provide a valid guess.</s>\n",
      "<|assistant|>\n",
      "Enter your next guess.\n",
      "[Reward]: -0.1\n",
      "[Info]: {'suffix': 'Enter your next guess.'}\n",
      "------------------------------\n",
      "\n",
      "========== Episode Finished ==========\n",
      "Total Reward: -0.1\n",
      "Final Info: {'suffix': 'Enter your next guess.'}\n",
      "Failed to run model TinyLlama/TinyLlama-1.1B-Chat-v1.0. Error: 'ObservationWrapper' object has no attribute 'close'\n",
      "\n",
      "All models processed.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Configuração do Ambiente Base (Sem Ferramentas) ---\n",
    "# Este é o ambiente principal com o problema (ex: GSM8K)\n",
    "base_env = gem.make(\"game:GuessTheNumber-v0-easy\")\n",
    "# Ferramentas removidas desta versão\n",
    "device = \"cpu\"\n",
    "# Lista de modelos para testar\n",
    "models = [\n",
    "    # \"microsoft/Phi-3-mini-4k-instruct\", # Usando Phi-3 no lugar do Phi-4 que não existe\n",
    "    # \"Qwen/Qwen2-0.5B-Instruct\",      # Usando Qwen2 0.5B no lugar do Qwen3 0.6B\n",
    "    # \"deepseek-ai/DeepSeek-R1\",\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # Usando um modelo de código Deepseek\n",
    "]\n",
    "\n",
    "# --- 2. Loop Principal (Iterar sobre cada modelo) ---\n",
    "for model_name in models:\n",
    "    print(f\"\\n{'='*20} Using model: {model_name} {'='*20}\")\n",
    "    \n",
    "    try:\n",
    "        # --- 3. Carregar Modelo e Tokenizer ---\n",
    "        # trust_remote_code=True é frequentemente necessário para modelos mais novos\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float32 # Usar float32 para CPU\n",
    "        ).to(device)\n",
    "        llm_model.eval() # Definir modelo para modo de avaliação\n",
    "\n",
    "        # --- 4. Aplicar o Wrapper de Chat ---\n",
    "        # Este wrapper formata o histórico de observações em um prompt de chat\n",
    "        # que o LLM entende.\n",
    "        # Alterado: 'tool_env' foi substituído por 'base_env'\n",
    "        chat_env = WRAPPER_FACTORY[\"concat_chat\"](\n",
    "            base_env, tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "        # --- 5. Iniciar o Episódio ---\n",
    "        obs, info = chat_env.reset()\n",
    "        print(f\"[Initial Observation (Problem)]:\\n{obs}\")\n",
    "        print(f\"[Initial Info]: {info}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        # --- 6. Loop de Interação (Agente-Ambiente) ---\n",
    "        while True:\n",
    "            step_count += 1\n",
    "            print(f\"\\n[Step {step_count}]\")\n",
    "\n",
    "            # === AÇÃO DO LLM ===\n",
    "            # Em vez de uma ação aleatória, geramos uma ação com o LLM\n",
    "            with torch.no_grad(): # Desativar cálculo de gradiente para inferência\n",
    "                action = generate_action(obs, llm_model, tokenizer, device)\n",
    "            \n",
    "            # --- LOG: Ação do LLM ---\n",
    "            print(f\"\\n[LLM Action]:\\n{action}\\n\")\n",
    "            \n",
    "            # === PASSO NO AMBIENTE ===\n",
    "            # O 'chat_env' recebe a ação de texto.\n",
    "            # Como não há ToolEnvWrapper, a ação será passada\n",
    "            # diretamente para o 'base_env' como a resposta final.\n",
    "            next_observation, reward, terminated, truncated, info = chat_env.step(action)\n",
    "            \n",
    "            # --- LOG: Feedback do Ambiente ---\n",
    "            print(f\"[Environment Feedback (Next Observation)]:\\n{next_observation}\")\n",
    "            print(f\"[Reward]: {reward}\")\n",
    "            print(f\"[Info]: {info}\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "            # Atualizar a observação para o próximo loop\n",
    "            obs = next_observation\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Condição de parada\n",
    "            if terminated or truncated:\n",
    "                print(f\"\\n{'='*10} Episode Finished {'='*10}\")\n",
    "                print(f\"Total Reward: {episode_reward}\")\n",
    "                print(f\"Final Info: {info}\")\n",
    "                break\n",
    "            \n",
    "            # Limite de segurança (opcional, mas bom para debug)\n",
    "            if step_count > 10:\n",
    "                print(\"Step limit reached, truncating episode.\")\n",
    "                break\n",
    "\n",
    "        chat_env.close()\n",
    "        print(f\"=== Finished using model: {model_name} ===\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to run model {model_name}. Error: {e}\")\n",
    "        # Limpar memória da GPU se houver erro antes de ir para o próximo modelo\n",
    "        if 'llm_model' in locals():\n",
    "            del llm_model\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nAll models processed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
